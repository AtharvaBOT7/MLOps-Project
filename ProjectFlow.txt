--------------------------------Setting up the project structure--------------------------------

1. Install cookiecutter using
    >> pip install cookiecutter
2.  Run the following command in your terminal to get a project structure template:
    >> cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science
3. Remove the project named folder after copying the folders and files in your main local project directory.
4. Rename src.models to src.model


--------------------------------Setting up MLFlow on Dagshub--------------------------------
5. Go to Dagshub dashboard on the internet.
6. Create a new repository and connect that repo to your github repository. 
    >> Create -> New Repo -> Connect a repo (Github) -> Connect -> Select the repo -> Connect
7. Added the IMDB.csv dataset to the notebooks folder

-------------------------Setup MLFlow on Dagshub---------------------------
8. Go to: https://dagshub.com/dashboard
9. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
10. Copy experiment tracking url and code snippet. (Also try: Go To MLFlow UI)
11. pip install dagshub & mlflow

12. Run the exp notebooks
13. git add - commit - push

14. dvc init
15. create a local folder as "local_s3" (temporary work)
16. on terminal - "dvc remote add -d mylocal local_s3"

17. Add code to below files/folders inside src dir:
    - logger
    - data_ingestion.py
    - data_preprocessing.py
    - feature_engineering.py
    - model_building.py
    - model_evaluation.py
    - register_model.py
18. add file - dvc.yaml (till model evaluation.metrics)
19. add file - params.yaml
20. DVC pipeline is ready to run - dvc repro
21. Once do - dvc status
22. git add - commit - push

23. Need to add S3 as remote storage - Create IAM User(keep cred) and S3 bucket
24. pip install - dvc[s3] & awscli
25. Checking/deleting dvc remote (optional) - [dvc remote list & dvc remote remove <name>] 
26. Set aws cred - aws configure
27. Add s3 as dvc remote storage - dvc remote add -d myremote s3://<bucket-name>

28. Create new dir - flask_app | Inside that, add rest of the files and dir
29. pip install flask and run the app (dvc push - to push data to S3)

30. pip freeze > requirements.txt
31. Add .github/workflows/ci.yaml file

32. Create key token on Dagshub for auth: Go to dagshub repo > Your settings > Tokens > Generate new token
    >> Please make sure to save token << >> capstone_test: <
    >> Add this auth token to github secret&var and update on ci file

31. Add dir "tests"&"scripts" and files within. This will contain our test related scripts for CI.

>>>>> Moving to Docker <<<<<

We will not push the entire codebase into Docker image because it will bloat the size of the image, therefore we have
created a flask_app folder and will push that folder to Docker, so that the size of the image is less but it performs 
as per expectation, also we have created a requirements file inside the flask_app directory to install only the necessary 
packages.

32. pip install pipreqs
33. cd flask_app & do "pipreqs . --force"
34. Add dockerfile and start docker-desktop in background
    Also before proceeding make sure: [switch the mlflow server setup to param version, change cmd on dockerfile]
35. go to root dir and: "docker build -t atlas:latest ."
36. Try running the image: "docker run -p 8888:5001 atlas:latest"
    - This run will give 'OSError: capstone_test environment variable is not set'...obviously
    - alternate: docker run -p 8888:5001 -e CAPSTONE_TEST=<your_dagshub_token> atlas:latest
    - docker push youruser/atlas:latest (optional)
    - optional: try to delete image locally and pull it from dockerhub and run (optional)
## This will run your program on localhost:8888

37. Setup aws services for below secrets and variables:
	AWS_ACCESS_KEY_ID
	AWS_SECRET_ACCESS_KEY
	AWS_REGION
	ECR_REPOSITORY (capstone-proj)
    AWS_ACCOUNT_ID
   (Also add this permission to the IAM user: AmazonEC2ContainerRegistryFullAccess)

38. Execute CICD pipeline till the stage where we build and push image to ECR.